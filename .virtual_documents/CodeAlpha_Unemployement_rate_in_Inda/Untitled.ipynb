# ğŸ“Š Unemployment Analysis  
# Professional notebook prepared for CodeAlpha Internship  
# **Author:** JosÃ© AntÃ´nio Afonso (Afonso)  
# **Date:** 2025-11-01 01:33 UTC

# ---

# 1ï¸âƒ£ Imports and settings
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.dates import DateFormatter
import matplotlib.dates as mdates
from statsmodels.tsa.seasonal import seasonal_decompose
from datetime import datetime

plt.style.use('default')
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 10
pd.set_option('display.max_columns', 50)
pd.set_option('display.width', 1000)

print("ğŸ“Š Unemployment Analysis - Task 2")
print("=" * 50)

# 2ï¸âƒ£ Load and Clean Data
def load_and_clean(path):
    """Load and clean unemployment dataset"""
    try:
        df = pd.read_csv(path)
        df.columns = df.columns.str.strip()
        
        # Detect and convert date column
        date_col = 'Date'
        if date_col not in df.columns:
            for c in df.columns:
                if 'date' in c.lower():
                    date_col = c
                    break
        
        # Convert date with proper error handling
        df[date_col] = pd.to_datetime(df[date_col], dayfirst=True, errors='coerce')
        df = df.dropna(subset=[date_col])
        
        # Detect unemployment column
        possible_cols = [c for c in df.columns if 'unemployment' in c.lower() or 'rate' in c.lower()]
        rate_col = possible_cols[0] if possible_cols else df.columns[-1]
        
        # Clean and convert unemployment rate
        df[rate_col] = (df[rate_col].astype(str)
                        .str.replace('%', '', regex=False)
                        .str.replace(',', '.', regex=False)
                        .astype(float))
        
        df = df.rename(columns={rate_col: 'UnemploymentRate', date_col: 'Date'})
        df = df.sort_values('Date').reset_index(drop=True)
        return df
        
    except Exception as e:
        print(f"Error loading {path}: {e}")
        return None

def load_and_process_data():
    """Load and combine all unemployment datasets"""
    paths = [
        "../data/Unemployment in India.csv",
        "../data/Unemployment_Rate_upto_11_2020.csv"
    ]
    
    dfs = []
    for p in paths:
        try:
            d = load_and_clean(p)
            if d is not None:
                print(f"âœ… Loaded {p} with shape {d.shape}")
                dfs.append(d)
            else:
                print(f"âŒ Failed to load {p}")
        except Exception as e:
            print(f"âŒ Error loading {p}: {e}")
    
    if not dfs:
        raise ValueError("No datasets were successfully loaded")
    
    # Combine datasets
    df = pd.concat(dfs, ignore_index=True).sort_values('Date').reset_index(drop=True)
    
    # Data quality checks
    print(f"âœ… Combined dataset shape: {df.shape}")
    print(f"âœ… Date range: {df['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')}")
    
    return df

# Load the data
df = load_and_process_data()

# Display basic info
print("\nğŸ“‹ Dataset Overview:")
print("=" * 40)
print(f"Total records: {len(df):,}")
print(f"Date range: {df['Date'].min().date()} to {df['Date'].max().date()}")
print(f"Regions: {df['Region'].nunique()}")
print(f"Areas: {df['Area'].nunique() if 'Area' in df.columns else 'N/A'}")

print("\nğŸ” Missing values:")
print("=" * 40)
missing_data = df.isnull().sum()
for col, missing in missing_data.items():
    if missing > 0:
        print(f"{col}: {missing} ({missing/len(df)*100:.1f}%)")

# Display first few rows
print("\nğŸ“„ First 5 rows:")
print("=" * 40)
display(df.head())

# 3ï¸âƒ£ Exploratory Data Analysis
print("\nğŸ“Š Exploratory Data Analysis")
print("=" * 50)

# Basic statistics
print("\nğŸ“ˆ Statistical Summary:")
print("=" * 40)
numeric_cols = ['UnemploymentRate', 'Estimated Employed', 'Estimated Labour Participation Rate (%)']
display(df[numeric_cols].describe())

# Check for duplicates
duplicates = df.duplicated().sum()
print(f"\nğŸ” Duplicate records: {duplicates}")

if duplicates > 0:
    df = df.drop_duplicates()
    print(f"âœ… Removed {duplicates} duplicates")

# 4ï¸âƒ£ National Trend and Yearly Analysis
print("\nğŸ‡®ğŸ‡³ National Trend Analysis")
print("=" * 50)

# Create a copy for analysis
analysis_df = df.copy()

# Extract year and month for time series analysis
analysis_df['Year'] = analysis_df['Date'].dt.year
analysis_df['Month'] = analysis_df['Date'].dt.month
analysis_df['YearMonth'] = analysis_df['Date'].dt.to_period('M')

# National unemployment trend
national_trend = analysis_df.groupby('Date')['UnemploymentRate'].mean().reset_index()

print("\nğŸ“… Yearly Analysis:")
print("=" * 40)
yearly_stats = analysis_df.groupby('Year').agg({
    'UnemploymentRate': ['mean', 'std', 'min', 'max'],
    'Estimated Employed': 'mean',
    'Estimated Labour Participation Rate (%)': 'mean'
}).round(2)

yearly_stats.columns = ['Avg_Rate', 'Std_Rate', 'Min_Rate', 'Max_Rate', 'Avg_Employed', 'Avg_Participation']
display(yearly_stats)

# 5ï¸âƒ£ Visualization
print("\nğŸ“Š Data Visualizations")
print("=" * 50)

# Create subplots
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('Unemployment Analysis Dashboard', fontsize=16, fontweight='bold')

# Plot 1: Unemployment Rate Over Time
axes[0, 0].plot(national_trend['Date'], national_trend['UnemploymentRate'], 
                color='teal', linewidth=2, marker='o', markersize=3)
axes[0, 0].set_title('National Unemployment Rate Trend', fontweight='bold')
axes[0, 0].set_xlabel('Date')
axes[0, 0].set_ylabel('Unemployment Rate (%)')
axes[0, 0].grid(True, alpha=0.3)
axes[0, 0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
axes[0, 0].tick_params(axis='x', rotation=45)

# Plot 2: Yearly Average Unemployment Rate
yearly_avg = analysis_df.groupby('Year')['UnemploymentRate'].mean()
axes[0, 1].bar(yearly_avg.index, yearly_avg.values, color='skyblue', alpha=0.7)
axes[0, 1].set_title('Average Unemployment Rate by Year', fontweight='bold')
axes[0, 1].set_xlabel('Year')
axes[0, 1].set_ylabel('Average Unemployment Rate (%)')
axes[0, 1].grid(True, alpha=0.3)

# Add value labels on bars
for i, v in enumerate(yearly_avg.values):
    axes[0, 1].text(i, v + 0.1, f'{v:.1f}%', ha='center', va='bottom')

# Plot 3: Regional Analysis (Top 10 regions by average unemployment)
if 'Region' in df.columns:
    regional_avg = analysis_df.groupby('Region')['UnemploymentRate'].mean().sort_values(ascending=False).head(10)
    axes[1, 0].barh(range(len(regional_avg)), regional_avg.values, color='lightcoral', alpha=0.7)
    axes[1, 0].set_yticks(range(len(regional_avg)))
    axes[1, 0].set_yticklabels(regional_avg.index)
    axes[1, 0].set_title('Top 10 Regions by Average Unemployment Rate', fontweight='bold')
    axes[1, 0].set_xlabel('Average Unemployment Rate (%)')
    
    # Add value labels
    for i, v in enumerate(regional_avg.values):
        axes[1, 0].text(v + 0.1, i, f'{v:.1f}%', va='center')

# Plot 4: Area-wise comparison (if available)
if 'Area' in df.columns and df['Area'].notna().sum() > 0:
    area_avg = analysis_df.groupby('Area')['UnemploymentRate'].mean()
    axes[1, 1].bar(area_avg.index, area_avg.values, color=['lightgreen', 'lightblue'], alpha=0.7)
    axes[1, 1].set_title('Average Unemployment Rate by Area', fontweight='bold')
    axes[1, 1].set_xlabel('Area')
    axes[1, 1].set_ylabel('Average Unemployment Rate (%)')
    
    # Add value labels
    for i, v in enumerate(area_avg.values):
        axes[1, 1].text(i, v + 0.1, f'{v:.1f}%', ha='center', va='bottom')

plt.tight_layout()
plt.show()

# 6ï¸âƒ£ Additional Analysis: Monthly Patterns
print("\nğŸ“… Monthly Pattern Analysis")
print("=" * 50)

monthly_pattern = analysis_df.groupby('Month')['UnemploymentRate'].mean()

plt.figure(figsize=(10, 6))
months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
plt.plot(months, monthly_pattern.values, marker='o', linewidth=2, color='purple')
plt.title('Monthly Pattern of Unemployment Rate', fontweight='bold')
plt.xlabel('Month')
plt.ylabel('Average Unemployment Rate (%)')
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 7ï¸âƒ£ Key Insights
print("\nğŸ’¡ Key Insights")
print("=" * 50)

# Calculate key metrics
current_rate = national_trend['UnemploymentRate'].iloc[-1]
max_rate = national_trend['UnemploymentRate'].max()
min_rate = national_trend['UnemploymentRate'].min()
avg_rate = national_trend['UnemploymentRate'].mean()

print(f"ğŸ“ˆ Current Unemployment Rate: {current_rate:.1f}%")
print(f"ğŸ“Š Average Rate (Overall): {avg_rate:.1f}%")
print(f"â¬†ï¸  Highest Recorded Rate: {max_rate:.1f}%")
print(f"â¬‡ï¸  Lowest Recorded Rate: {min_rate:.1f}%")
print(f"ğŸ“… Data covers {len(analysis_df['Year'].unique())} years")

if 'Region' in df.columns:
    highest_region = analysis_df.groupby('Region')['UnemploymentRate'].mean().idxmax()
    lowest_region = analysis_df.groupby('Region')['UnemploymentRate'].mean().idxmin()
    print(f"ğŸ† Region with highest unemployment: {highest_region}")
    print(f"ğŸ¯ Region with lowest unemployment: {lowest_region}")

# 8ï¸âƒ£ Data Quality Assessment
print("\nğŸ” Data Quality Assessment")
print("=" * 50)

# Check for outliers in unemployment rate
Q1 = df['UnemploymentRate'].quantile(0.25)
Q3 = df['UnemploymentRate'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df[(df['UnemploymentRate'] < lower_bound) | (df['UnemploymentRate'] > upper_bound)]
print(f"ğŸ“Š Potential outliers in Unemployment Rate: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)")

if len(outliers) > 0:
    print("\nğŸ” Sample outliers:")
    display(outliers[['Region', 'Date', 'UnemploymentRate']].head())

print("\nâœ… Analysis completed successfully!")
print("=" * 50)



