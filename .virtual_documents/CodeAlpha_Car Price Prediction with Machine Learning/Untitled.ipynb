


# Bibliotecas bÃ¡sicas
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# PrÃ©-processamento
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Modelos de RegressÃ£o
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR

# MÃ©tricas de AvaliaÃ§Ã£o
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ConfiguraÃ§Ã£o
plt.style.use('seaborn-v0_8')
%matplotlib inline





# Carregar dataset (ajusta o nome do ficheiro)
# Supondo 'car_data.csv' ou similar
df = pd.read_csv('../data/car data.csv')

# ExploraÃ§Ã£o inicial
print("=== EXPLORAÃ‡ÃƒO INICIAL DO DATASET ===")
print(f"Forma do dataset: {df.shape}")
print(f"\nPrimeiras 5 linhas:")
print(df.head())

print(f"\nInformaÃ§Ãµes do dataset:")
print(df.info())

print(f"\nEstatÃ­sticas descritivas:")
print(df.describe())

print(f"\nValores nulos por coluna:")
print(df.isnull().sum())

# Verificar a coluna de preÃ§o (target)
print(f"\nColunas disponÃ­veis: {df.columns.tolist()}")





# ConfiguraÃ§Ã£o para visualizaÃ§Ãµes
plt.figure(figsize=(15, 12))

# 1. DistribuiÃ§Ã£o do preÃ§o (target)
plt.subplot(2, 3, 1)
plt.hist(df['Present_Price'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('DistribuiÃ§Ã£o dos PreÃ§os dos Carros')
plt.xlabel('PreÃ§o')
plt.ylabel('FrequÃªncia')
plt.grid(True, alpha=0.3)

# 2. PreÃ§o vs Ano
plt.subplot(2, 3, 2)
plt.scatter(df['Year'], df['Selling_Price'], alpha=0.6, color='green')
plt.title('PreÃ§o vs Ano do Carro')
plt.xlabel('Ano')
plt.ylabel('PreÃ§o')
plt.grid(True, alpha=0.3)

# 3. PreÃ§o vs Quilometragem
plt.subplot(2, 3, 3)
plt.scatter(df['Driven_kms'], df['Selling_Price'], alpha=0.6, color='red')
plt.title('PreÃ§o vs Quilometragem')
plt.xlabel('Quilometragem')
plt.ylabel('PreÃ§o')
plt.grid(True, alpha=0.3)

# 4. PreÃ§o por Tipo de CombustÃ­vel
plt.subplot(2, 3, 4)
if 'Fuel_Type' in df.columns:
    sns.boxplot(data=df, x='Fuel_Type', y='Present_Price', palette='Set2')
    plt.title('PreÃ§o por Tipo de CombustÃ­vel')
    plt.xticks(rotation=45)

# 5. PreÃ§o por TransmissÃ£o
plt.subplot(2, 3, 5)
if 'Transmission' in df.columns:
    sns.boxplot(data=df, x='Transmission', y='Present_Price', palette='Set3')
    plt.title('PreÃ§o por Tipo de TransmissÃ£o')

# 6. PreÃ§o por Marca (top 10)
plt.subplot(2, 3, 6)
if 'Car_Name' in df.columns or 'company' in df.columns:
    brand_col = 'Car_Name' if 'Car_Name' in df.columns else 'company'
    top_brands = df[brand_col].value_counts().head(10).index
    top_brands_data = df[df[brand_col].isin(top_brands)]
    sns.boxplot(data=top_brands_data, x=brand_col, y='Present_Price', palette='viridis')
    plt.title('PreÃ§o por Marca (Top 10)')
    plt.xticks(rotation=45)

plt.tight_layout()
plt.show()





# Matriz de correlaÃ§Ã£o para variÃ¡veis numÃ©ricas
plt.figure(figsize=(10, 8))

# Selecionar apenas colunas numÃ©ricas
numeric_cols = df.select_dtypes(include=[np.number]).columns

if len(numeric_cols) > 1:
    correlation_matrix = df[numeric_cols].corr()
    
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                square=True, linewidths=0.5, fmt='.2f')
    plt.title('Matriz de CorrelaÃ§Ã£o entre VariÃ¡veis NumÃ©ricas')
    plt.tight_layout()
    plt.show()
    
    # CorrelaÃ§Ã£o com preÃ§o
    if 'Selling_Price' in correlation_matrix.columns:
        print("Correlation with price:")
        price_corr = correlation_matrix['Selling_Price'].sort_values(ascending=False)
        print(price_corr)





# Identificar tipos de colunas
def prepare_features(df):
    # Fazer cÃ³pia para nÃ£o modificar original
    df_clean = df.copy()
    
    # 1. Lidar com valores nulos
    print("Valores nulos antes do tratamento:")
    print(df_clean.isnull().sum())
    
    # Preencher numÃ©ricos com mediana, categÃ³ricos com moda
    for col in df_clean.columns:
        if df_clean[col].isnull().sum() > 0:
            if df_clean[col].dtype in ['int64', 'float64']:
                df_clean[col].fillna(df_clean[col].median(), inplace=True)
            else:
                df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)
    
    # 2. Separar features e target
    X = df_clean.drop('Selling_Price', axis=1)  # Todas as colunas exceto preÃ§o
    y = df_clean['Selling_Price']  # Target
    
    return X, y

X, y = prepare_features(df)

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
print(f"\nColunas em X: {X.columns.tolist()}")





# Identificar colunas numÃ©ricas e categÃ³ricas
numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(include=['object']).columns.tolist()

print(f"Features numÃ©ricas: {numeric_features}")
print(f"Features categÃ³ricas: {categorical_features}")

# Criar prÃ©-processador
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)
    ])

# Dividir em treino e teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

print(f"Treino: {X_train.shape[0]} amostras")
print(f"Teste: {X_test.shape[0]} amostras")





# Definir modelos a testar
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Lasso Regression': Lasso(alpha=1.0),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'Support Vector Regression': SVR(kernel='rbf')
}

# Treinar e avaliar cada modelo
results = {}

for name, model in models.items():
    print(f"\nTreinando {name}...")
    
    # Criar pipeline com prÃ©-processamento + modelo
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('model', model)
    ])
    
    # Treinar modelo
    pipeline.fit(X_train, y_train)
    
    # Fazer previsÃµes
    y_pred = pipeline.predict(X_test)
    
    # Calcular mÃ©tricas
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    
    # Guardar resultados
    results[name] = {
        'model': pipeline,
        'mae': mae,
        'mse': mse,
        'rmse': rmse,
        'r2': r2,
        'predictions': y_pred
    }
    
    print(f"{name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, RÂ²: {r2:.4f}")





# Comparar performance dos modelos
comparison_df = pd.DataFrame({
    'Model': list(results.keys()),
    'MAE': [results[name]['mae'] for name in results.keys()],
    'RMSE': [results[name]['rmse'] for name in results.keys()],
    'RÂ²': [results[name]['r2'] for name in results.keys()]
}).sort_values('RMSE')

print("\n" + "="*60)
print("COMPARAÃ‡ÃƒO DE MODELOS (ordenado por RMSE)")
print("="*60)
print(comparison_df)

# VisualizaÃ§Ã£o da comparaÃ§Ã£o
plt.figure(figsize=(12, 8))

# Subplot 1: ComparaÃ§Ã£o de RÂ²
plt.subplot(2, 2, 1)
plt.barh(comparison_df['Model'], comparison_df['RÂ²'], color='lightgreen')
plt.title('ComparaÃ§Ã£o de RÂ² entre Modelos')
plt.xlabel('RÂ² Score')
plt.xlim(0, 1)

# Subplot 2: ComparaÃ§Ã£o de RMSE
plt.subplot(2, 2, 2)
plt.barh(comparison_df['Model'], comparison_df['RMSE'], color='lightcoral')
plt.title('ComparaÃ§Ã£o de RMSE entre Modelos')
plt.xlabel('RMSE')

# Subplot 3: PrevisÃµes vs Valores Reais (melhor modelo)
best_model_name = comparison_df.iloc[0]['Model']
best_model = results[best_model_name]['model']
y_pred_best = results[best_model_name]['predictions']

plt.subplot(2, 2, 3)
plt.scatter(y_test, y_pred_best, alpha=0.6, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Valores Reais')
plt.ylabel('PrevisÃµes')
plt.title(f'PrevisÃµes vs Reais - {best_model_name}')

# Subplot 4: ResÃ­duos
plt.subplot(2, 2, 4)
residuals = y_test - y_pred_best
plt.scatter(y_pred_best, residuals, alpha=0.6, color='purple')
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel('PrevisÃµes')
plt.ylabel('ResÃ­duos')
plt.title('AnÃ¡lise de ResÃ­duos')

plt.tight_layout()
plt.show()





# FunÃ§Ã£o para fazer previsÃµes em novos carros
def predict_car_price(model, car_features):
    """
    Prediz o preÃ§o de um carro com base nas caracterÃ­sticas
    """
    # Criar DataFrame com as features
    new_car_df = pd.DataFrame([car_features])
    
    # Fazer previsÃ£o
    predicted_price = model.predict(new_car_df)[0]
    
    return predicted_price

# Exemplo de uso com o melhor modelo
if 'best_model_name' in locals():
    # Exemplo de caracterÃ­sticas de um carro
    example_car = {
        'Year': 2020,
        'Driven_kms': 15000,
        'fuel_type': 'Petrol',
        'Transmission': 'Manual',
        'Car_Name': 'Toyota',
        'Present_Price':20.9,
        'Selling_type':'Deal',
        'Transmission':'Manual',
        'Owner':0,
        'Fuel_Type':'Diesel'
        # Adiciona mais caracterÃ­sticas conforme teu dataset
    }
    
    predicted_price = predict_car_price(best_model, example_car)
    print(f"\nEXEMPLO DE PREVISÃƒO:")
    print(f"CaracterÃ­sticas: {example_car}")
    print(f"PreÃ§o previsto: â‚¬{predicted_price:,.2f}")





print("="*70)
print("RELATÃ“RIO FINAL - PREVISÃƒO DE PREÃ‡OS DE CARROS")
print("="*70)

print(f"ðŸ“Š PERFORMANCE DO MELHOR MODELO ({best_model_name}):")
print(f"   â€¢ RÂ² Score: {results[best_model_name]['r2']:.4f}")
print(f"   â€¢ MAE: â‚¬{results[best_model_name]['mae']:,.2f}")
print(f"   â€¢ RMSE: â‚¬{results[best_model_name]['rmse']:,.2f}")

print(f"\nðŸ” INSIGHTS DO MODELO:")
print(f"   â€¢ Total de features utilizadas: {len(X.columns)}")
print(f"   â€¢ Features numÃ©ricas: {len(numeric_features)}")
print(f"   â€¢ Features categÃ³ricas: {len(categorical_features)}")

print(f"\nðŸ’¡ INTERPRETAÃ‡ÃƒO DO RÂ² SCORE:")
r2_score_value = results[best_model_name]['r2']
if r2_score_value > 0.8:
    print("   â€¢ Excelente! O modelo explica mais de 80% da variabilidade dos preÃ§os")
elif r2_score_value > 0.6:
    print("   â€¢ Bom! O modelo explica mais de 60% da variabilidade dos preÃ§os")
else:
    print("   â€¢ Pode ser melhorado. Considera feature engineering adicional")

print(f"\nðŸš— FATORES QUE MAIS INFLUENCIAM O PREÃ‡O:")
if 'feature_importance_df' in locals():
    top_factors = feature_importance_df.head(3)['feature'].tolist()
    for i, factor in enumerate(top_factors, 1):
        print(f"   {i}. {factor}")

print(f"\nðŸ“ˆ RECOMENDAÃ‡Ã•ES:")
print("   â€¢ Para melhorar o modelo: coletar mais dados, especialmente de carros raros")
print("   â€¢ Considerar interaÃ§Ãµes entre features (ex: ano + marca)")
print("   â€¢ Testar modelos mais complexos como XGBoost ou Neural Networks")



